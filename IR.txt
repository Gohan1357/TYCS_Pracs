#PRAC 1- BITWISE OPERATION 

def bitwise_op(a, b):
    bit_and = a & b
    bit_or = a | b
    bit_a_not = ~a
    bit_b_not = ~b
    bit_xor = a ^ b
    rshift_a = a >> 1
    rshift_b = b >> 1
    lshift_a = 1 << a
    lshift_b = 1 << b


    return (bit_and, bit_or, bit_a_not, bit_b_not, bit_xor, rshift_a, rshift_b, lshift_a, lshift_b)


a = 1000
b = 1110
results = bitwise_op(a, b)


print(f'BITWISE AND: \t{results[0]}\n')
print(f'BITWISE OR: \t{results[1]}\n')
print(f'BITWISE NOT A: \t{results[2]}\n')
print(f'BITWISE NOT B: \t{results[3]}\n')
print(f'BITWISE XOR: \t{results[4]}\n')
print(f'BITWISE RSHIFT A: \t{results[5]}\n')
print(f'BITWISE RSHIFT B: \t{results[6]}\n')
print(f'BITWISE LSHIFT A: \t{results[7]}\n')
print(f'BITWISE LSHIFT B: \t{results[8]}\n')






PRAC 2: PAGE RANK IMPLEMENTATION:
#Method 1 : Implementation of PageRank using NetworkX without weighted edge
import networkx as nx
import pylab as plt
G=nx.DiGraph()
[G.add_node(k) for k in ["A","B","C","D","E","F","G"]]
G.add_edges_from([("F","A"),("G","A"),("A","G"),("B","A"),("C","A"),("A","C"),("E","A"),("D","B"),("D","F "),("A","D")])


ppr1=nx.pagerank(G)
print("Page Rank Value : ",ppr1)
pos=nx.spiral_layout(G)
nx.draw_networkx(G,pos,with_labels=True , node_color="#f86e00")
plt.show()




#Method 2 : Implementation of PageRank using weighted edge


import networkx as nx
import pylab as plt
D=nx.DiGraph()
D.add_weighted_edges_from([('A','B',1),('A','C',1),('C','A',1),('B','C',1)])
ppr1=nx.pagerank(D)
print("Page Rank Value : ",ppr1)
pos=nx.spiral_layout(D)
nx.draw_networkx(D,pos,with_labels=True , node_color="#f86e00")
plt.show()


























# Implementation using formula
def page_rank(graph, damping_factor=0.85, max_iterations=100, tolerance=1e-6):
    num_pages = len(graph)
    initial_page_rank = 1.0 / num_pages
    page_ranks = {page: initial_page_rank for page in graph}


    for _ in range(max_iterations):
        new_page_ranks = {}
        for page in graph:
            new_rank = (1 - damping_factor) / num_pages
            for link in graph:
                if page in graph[link]:
                    new_rank += damping_factor * (page_ranks[link] / len(graph[link]))
            new_page_ranks[page] = new_rank
       
        # Check convergence
        convergence = all(abs(new_page_ranks[page] - page_ranks[page]) < tolerance for page in graph)
       
        # Update page ranks
        page_ranks = new_page_ranks
       
        if convergence:
            break


    return page_ranks


if __name__ == "__main__":
    example_graph = {
        'A': ['B', 'C'],
        'B': ['A'],
        'C': ['A', 'B'],
        'D': ['B']
    }
    result = page_rank(example_graph)
    for page, rank in sorted(result.items(), key=lambda x: x[1], reverse=True):
        print(f"Page: {page} - PageRank: {rank:.4f}")






PRAC 3:Levenstein Distance 

def leven(x,y):
    n=len(x)
    m=len(y)
    A=[[i+j for j in range(m+1)] for i in range(n+1)]
    for i in range(n):
        for j in range (m):
            A[i+1][j+1]=min(A[i][j+1]+1, A[i+1][j]+1,A[i][j]+int(x[i]!=y[j]))
    return A[n][m]
print(leven("brap","rap"))
print(leven("trial","try"))
print(leven("horse","force"))
print(leven("rose","erode"))



Prac 4 A- Jaccard Similarity 
# PRACTICAL 4.1
# JACCARD SIMILARITY


def Jacard_Similarity(doc1,doc2):
    words_doc1 = set(doc1.lower().split())
    words_doc2 = set(doc2.lower().split())
    intersection = words_doc1.intersection(words_doc2)
    union=words_doc1.union(words_doc2)
    return float(len(intersection))/len(union)
doc1="data is the new oil of the digital economy"
doc2="data is the new oil"
Jacard_Similarity(doc1,doc2)



Prac 4b) Cosine Simiilarity



# PRACTICAL 4.2
# CONSINE SIMILARITY
 
import math


def dot_product(vector1, vector2):
    return sum(x * y for x, y in zip(vector1, vector2))


def magnitude(vector):
    return math.sqrt(sum(x**2 for x in vector))


def cosine_similarity(vector1, vector2):
    dot_prod = dot_product(vector1, vector2)
    mag_vector1 = magnitude(vector1)
    mag_vector2 = magnitude(vector2)
    if mag_vector1 == 0 or mag_vector2 == 0:
        return 0 # Avoid division by zero
    return dot_prod / (mag_vector1 * mag_vector2)
def main():
    # Example vectors (replace with your own vectors)
    vector1 = [2, 3, 5, 8, 13]
    vector2 = [1, 4, 6, 7, 14]
    similarity = cosine_similarity(vector1, vector2)
    print(f"Cosine Similarity: {similarity}")


if __name__== "__main__":
    main()




Prac 5- Map Reduce 

# PRACTICAL 5
# MAP REDUCE


from functools import reduce
from collections import defaultdict


def mapper(data):
    char_count=defaultdict(int)
    for char in data:
        if char.isalpha():
            char_count[char.lower()]+=1
    return char_count.items()
def reducer(counts1,counts2):
    merged_counts=defaultdict(int)
    for char, count in counts1:
        merged_counts[char]+=count
    for char,count in counts2:
        merged_counts[char]+=count
    return merged_counts.items()


if __name__=="__main__":
    #
    dataset="Hello, World! This is a Map reduce Example."
    # Map step
    chunks=[chunk for chunk in dataset.split()]
    #final Map step
    mapped_results=map(mapper,chunks)
    # reduce step
    final_counts=reduce(reducer,mapped_results)
    for char, count in final_counts:
        print(f"Character :{char} , Count: {count}")




Prac 6- HITS Algorithm 

#HITs ALGORITHM
import networkx as nx
import pylab as plt  # Importing pylab for visualization


# Step 1: Create a graph
G = nx.DiGraph()


# Step 2: Add edges to the graph
G.add_edges_from([(1, 2), (1, 3), (2, 4), (3, 4), (4, 5)])


# Step 3: Calculate the HITS scores
authority_scores, hub_scores = nx.hits(G)


# Step 4: Print the scores
print("Authority Scores:", authority_scores)
print("Hub Scores:", hub_scores)


# Optionally, visualize the graph
nx.draw(G, with_labels=True)
plt.show()



PRAC 7 : Nltk StopWords 
#PRACTICAL 7
# To Download Stopwords
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
set(stopwords.words('english'))
# To tokenize & Filter out sentence
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


example_sent="This is a sample sentence , showing off the stopwords filtration"
stop_words=set(stopwords.words('english'))
word_tokens=word_tokenize(example_sent)
filtered_sentence=[w for w in word_tokens if not w in stop_words]
filtered_sentence =[]


for w in word_tokens:
    if w not in stop_words:
        filtered_sentence.append(w)
       
print(word_tokens)
print(filtered_sentence)



PRAC 8:Twitter Minning 
#PRACTICAL 8


import pandas as pd
from ntscraper import Nitter
scraper=Nitter()


twitter=scraper.get_tweets('narendramodi',mode='user',number=5)
final_tweets=[]
for tweet in twitter['tweets']:
    data=[tweet['link'],tweet['text'],tweet['date'],tweet['stats']['likes']]
    final_tweets.append(data)
final_tweets


data=pd.DataFrame(final_tweets,columns=['link','text','data','number'])
data





PRAC 9:WEB CRAWLING


import requests
from parsel import Selector
import time
start=time.time()
response=requests.get('http://recurship.com/')
selector=Selector(response.text)
href_links=selector.xpath('//a/@href').getall()
image_links=selector.xpath('//img/@src').getall()
print("***********Href_links***********")
print(href_links)
print("***********/href_links***********")
print(image_links)
print("***********/image_links***********")
end=time.time()
print("Time Taken in seconds:",(end-start))






PRAC 10: XML RETRIEVAL 


XML Retrieval - topic specific page rank and person information 
import xml.etree.ElementTree as ET
import networkx as nx
def parse_xml(xml_text):
    root=ET.fromstring(xml_text)
    return root


def generate_web_graph(xml_root):
    G=nx.DiGraph()
   
    #add node
    for page in xml_root.findall('.//page'):
        page_id=page.find('id').text
        G.add_node(page_id)
       
        #edge - for connecting nodes
        links=page.findall('.//link')
        for link in links:
            target_page_id=link.text
            G.add_edge(page_id,target_page_id)
        return G
   
def compute_topic_specific_pagerank(graph,topic_nodes,alpha=0.85,max_iter=100,tol=1e-6):
    personalization={node:1.0 if node in topic_nodes else 0.0 for node in graph.nodes}
    return nx.pagerank(graph,alpha=alpha,personalization=personalization,max_iter=max_iter,tol=tol)


if __name__=="__main__":
    example_xml="""
    <webgraph>
        <page>
            <id>1</id>
            <link>2</link>
            <link>3</link>
        </page>
        <page>
            <id>2</id>
            <link>1</link>
            <link>2</link>
        </page>
        <page>
            <id>3</id>
            <link>1</link>
            <link>3</link>
        </page>
        </webgraph>
        """
    xml_root=parse_xml(example_xml)
    web_graph=generate_web_graph(xml_root)
    topic_specific_pagerank=compute_topic_specific_pagerank(web_graph,topic_nodes=['1','2'])
   
    print("topc - specific pagerank")
    for node,score in sorted(topic_specific_pagerank.items(),key=lambda x:x[1],reverse=True):
        print(f"Node:{node}-PageRank:{score:4f}")








#Print person details 
import xml.etree.ElementTree as ET
xml_data='''<root>
    <person>
        <name>John</name>
        <age>30</age>
        <city>Mumbai</city>
        </person>
    </root>    
        '''
tree= ET.fromstring(xml_data)
for person in tree.findall('person'):
    name=person.find('name').text
    age=person.find('age').text
    city=person.find('city').text
    print(f"Name:{name},Age:{age},City: {city}")







